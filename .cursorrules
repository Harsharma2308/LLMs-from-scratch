# LLMs from Scratch - Cursor Rules

## Current Status
- ‚úÖ Conda environment `llms-scratch` created with PyTorch 2.10.0.dev+cu128
- ‚úÖ Basic training pipeline tested with TensorBoard integration 
- ‚úÖ Downloaded "The Verdict" dataset (5K tokens) for Chapter 5
- ‚úÖ Created 2.8MB training dataset from 5 classic books (777K tokens)
- ‚úÖ Trained GPT-124M model: 11‚Üí5 loss in 2.5 minutes (saved checkpoint: 623MB)
- ‚úÖ Integrated W&B + TensorBoard dual logging system (both enabled by default)
- ‚úÖ Fixed W&B step counter bug (now synced with TensorBoard: 0, 100, 200...)
- ‚úÖ Fixed Unicode text display in W&B (clean ASCII conversion for quotes/punctuation)
- ‚úÖ Fixed timing display (now shows per-eval time, not cumulative)
- ‚úÖ Fixed CUDA issue - resolved with system reboot (RTX 3500 GPU working!)
- ‚úÖ Added GPU requirement check (fails with clear error if no GPU)
- ‚úÖ Added --force-cpu flag for testing (exits after 1 step to prevent long waits)
- ‚úÖ Explored loading OpenAI GPT-2 pretrained weights (4 methods)
- ‚úÖ Downloaded and tested GPT-2 124M weights from HuggingFace (703MB)
- ‚úÖ Compared our model (777K tokens) vs OpenAI's (40GB text) - huge quality difference!
- ‚úÖ Created comprehensive guide: PRETRAINED_WEIGHTS_GUIDE.md
- üîÑ Working on Chapter 5: Understanding weight conversion and model architecture
- üìù Next: Fine-tuning pretrained models (Chapter 6)

## Project Context
Building Large Language Models from scratch following Sebastian Raschka's book. This is a fork of the main repository focused on educational implementation of GPT-like models using PyTorch.

## Development Environment
- **Framework**: PyTorch (2.2.2+)
- **Environment**: Conda
- **Conda Environment Name**: `llms-scratch`
- **Python**: 3.10-3.13
- **Main Dependencies**: tiktoken, matplotlib, tensorflow (for comparisons), tqdm, numpy, pandas

## Environment Setup
To activate the conda environment:
```bash
conda activate llms-scratch
```

All code should be run within this environment to ensure dependency compatibility.

## Code Standards
- Follow existing code patterns from the book chapters
- Use clear, educational variable names that match the book's terminology
- Add comments explaining mathematical concepts and model architecture decisions
- Prefer readability over optimization (educational codebase)

## Project Goals
1. Implement GPT-like models from scratch (focusing on 124M parameter model)
2. Integrate experiment tracking (W&B/TensorBoard) for loss visualization
3. Load and compare with OpenAI's pretrained GPT-2 weights
4. Focus on Chapter 5 (pretraining) with comprehensive understanding

## File Structure Conventions
- Main chapter code in `chXX/01_main-chapter-code/`
- Bonus materials in numbered subdirectories
- Keep notebooks (`.ipynb`) for exploration and scripts (`.py`) for production code
- Use `previous_chapters.py` to import utilities from earlier chapters

## Key Implementation Notes
- Tokenization: Use tiktoken/BPE tokenizer
- Model Architecture: Multi-head attention, layer normalization, feed-forward networks
- Training: AdamW optimizer, learning rate scheduling, gradient clipping
- Data: Text data from Project Gutenberg, tokenized and batched

## Testing & Validation
- Each chapter has `tests.py` for verification
- Exercise solutions available in `exercise-solutions.ipynb`
- Compare outputs with reference implementations

## Key Learnings About Pretrained Weights
- **PyTorch State Dict Method**: Recommended - use pre-converted weights from HuggingFace
- **Weight URL**: `https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/gpt2-small-124M.pth`
- **Key Transformations**: Split QKV matrices, transpose weights, remap parameter names
- **Quality Gap**: Pretrained (40GB data) vs our model (777K tokens) shows massive difference
- **Bias Note**: OpenAI uses `qkv_bias=True`, we typically use `False`

## Future Integrations
- Wandb/TensorBoard logging for:
  - Training/validation losses
  - Learning rate schedules
  - Gradient norms
  - Sample generations
- Model checkpointing during training
- Performance profiling and optimization tracking
- Fine-tuning pretrained models on custom datasets

## Notes
- Keep the "Current Status" section updated as progress is made
- This helps track where we are in the learning journey and what's next
