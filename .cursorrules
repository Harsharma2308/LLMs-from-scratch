# LLMs from Scratch - Cursor Rules

## Current Status
- ‚úÖ Conda environment `llms-scratch` created with PyTorch 2.8.0 + CUDA 12.8
- ‚úÖ Basic training pipeline tested with TensorBoard integration 
- ‚úÖ Downloaded "The Verdict" dataset (5K tokens) for Chapter 5
- ‚úÖ Created 2.8MB training dataset from 5 classic books (777K tokens)
- ‚úÖ Trained GPT-124M model: 11‚Üí5 loss in 2.6 minutes (saved checkpoint: 623MB)
- ‚úÖ Integrated W&B + TensorBoard dual logging system (both enabled by default)
- ‚úÖ Fixed W&B step counter bug (now synced with TensorBoard: 0, 100, 200...)
- ‚úÖ Fixed Unicode text display in W&B (no more garbled quotes/punctuation)
- üîÑ Working on Chapter 5: Understanding pretraining mechanics
- üìù Next: Load OpenAI GPT-2 weights and compare with our training

## Project Context
Building Large Language Models from scratch following Sebastian Raschka's book. This is a fork of the main repository focused on educational implementation of GPT-like models using PyTorch.

## Development Environment
- **Framework**: PyTorch (2.2.2+)
- **Environment**: Conda
- **Conda Environment Name**: `llms-scratch`
- **Python**: 3.10-3.13
- **Main Dependencies**: tiktoken, matplotlib, tensorflow (for comparisons), tqdm, numpy, pandas

## Environment Setup
To activate the conda environment:
```bash
conda activate llms-scratch
```

All code should be run within this environment to ensure dependency compatibility.

## Code Standards
- Follow existing code patterns from the book chapters
- Use clear, educational variable names that match the book's terminology
- Add comments explaining mathematical concepts and model architecture decisions
- Prefer readability over optimization (educational codebase)

## Project Goals
1. Implement GPT-like models from scratch (focusing on 124M parameter model)
2. Integrate experiment tracking (W&B/TensorBoard) for loss visualization
3. Load and compare with OpenAI's pretrained GPT-2 weights
4. Focus on Chapter 5 (pretraining) with comprehensive understanding

## File Structure Conventions
- Main chapter code in `chXX/01_main-chapter-code/`
- Bonus materials in numbered subdirectories
- Keep notebooks (`.ipynb`) for exploration and scripts (`.py`) for production code
- Use `previous_chapters.py` to import utilities from earlier chapters

## Key Implementation Notes
- Tokenization: Use tiktoken/BPE tokenizer
- Model Architecture: Multi-head attention, layer normalization, feed-forward networks
- Training: AdamW optimizer, learning rate scheduling, gradient clipping
- Data: Text data from Project Gutenberg, tokenized and batched

## Testing & Validation
- Each chapter has `tests.py` for verification
- Exercise solutions available in `exercise-solutions.ipynb`
- Compare outputs with reference implementations

## Future Integrations
- Wandb/TensorBoard logging for:
  - Training/validation losses
  - Learning rate schedules
  - Gradient norms
  - Sample generations
- Model checkpointing during training
- Performance profiling and optimization tracking

## Notes
- Keep the "Current Status" section updated as progress is made
- This helps track where we are in the learning journey and what's next
